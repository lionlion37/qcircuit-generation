{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Overfit Training Comparison (Small Dataset)\n",
        "\n",
        "This notebook runs a matrix of training configs, evaluates each run on fixed guidance scales, and compares:\n",
        "- decode validity (`decoded / samples`)\n",
        "- SRV exact match (on decoded subset)\n",
        "- training loss summary\n",
        "\n",
        "It is designed to quickly find a *working* small-dataset configuration before long runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import shlex\n",
        "import subprocess\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths\n",
        "REPO_ROOT = Path('..').resolve()\n",
        "TRAIN_SCRIPT = REPO_ROOT / 'scripts' / 'train_model.py'\n",
        "EVAL_SCRIPT = REPO_ROOT / 'scripts' / 'evaluate_model.py'\n",
        "RESULTS_DIR = REPO_ROOT / 'notebooks' / 'results' / 'overfit_training_comparison'\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Control flags\n",
        "RUN_TRAINING = True\n",
        "RUN_EVALUATION = True\n",
        "DRY_RUN = False\n",
        "\n",
        "# Base configs\n",
        "TRAIN_BASE = 'overfit_debug_srv'\n",
        "EVAL_BASE = 'overfit_debug_srv'\n",
        "\n",
        "# Evaluation sweep\n",
        "GUIDANCE_SCALES = [0.5, 1.0, 1.5, 3.0, 5.0, 10.0]\n",
        "NUM_EVAL_SAMPLES = 128\n",
        "\n",
        "STAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "print('Results dir:', RESULTS_DIR)\n",
        "print('Timestamp  :', STAMP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment matrix (edit freely)\n",
        "EXPERIMENTS = [\n",
        "    {\n",
        "        'name': 'exp_a_lr3e4_s512_e100_ip0p0',\n",
        "        'train_overrides': {\n",
        "            'training.learning_rate': '3e-4',\n",
        "            'training.max_samples': '512',\n",
        "            'training.num_epochs': '100',\n",
        "            'training.batch_size': '32',\n",
        "            'scheduler.params.input_perturbation': '0.0',\n",
        "            'training.enable_guidance_train': 'true',\n",
        "            'training.guidance_train_p': '0.1',\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        'name': 'exp_b_lr1e4_s512_e100_ip0p0',\n",
        "        'train_overrides': {\n",
        "            'training.learning_rate': '1e-4',\n",
        "            'training.max_samples': '512',\n",
        "            'training.num_epochs': '100',\n",
        "            'training.batch_size': '32',\n",
        "            'scheduler.params.input_perturbation': '0.0',\n",
        "            'training.enable_guidance_train': 'true',\n",
        "            'training.guidance_train_p': '0.1',\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        'name': 'exp_c_lr3e4_s1024_e80_ip0p0',\n",
        "        'train_overrides': {\n",
        "            'training.learning_rate': '3e-4',\n",
        "            'training.max_samples': '1024',\n",
        "            'training.num_epochs': '80',\n",
        "            'training.batch_size': '64',\n",
        "            'scheduler.params.input_perturbation': '0.0',\n",
        "            'training.enable_guidance_train': 'true',\n",
        "            'training.guidance_train_p': '0.1',\n",
        "        },\n",
        "    },\n",
        "]\n",
        "\n",
        "for e in EXPERIMENTS:\n",
        "    print('-', e['name'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_cmd(cmd, cwd=REPO_ROOT, env=None, dry_run=False):\n",
        "    printable = ' '.join(shlex.quote(str(x)) for x in cmd)\n",
        "    print('\\n$ ' + printable)\n",
        "    if dry_run:\n",
        "        return 0, '', ''\n",
        "\n",
        "    proc = subprocess.run(\n",
        "        cmd,\n",
        "        cwd=str(cwd),\n",
        "        env=env,\n",
        "        text=True,\n",
        "        capture_output=True,\n",
        "    )\n",
        "    if proc.returncode != 0:\n",
        "        print(proc.stdout[-2000:])\n",
        "        print(proc.stderr[-2000:])\n",
        "    return proc.returncode, proc.stdout, proc.stderr\n",
        "\n",
        "\n",
        "def parse_eval_stdout(text):\n",
        "    out = {\n",
        "        'samples_requested': np.nan,\n",
        "        'decoded_circuits': np.nan,\n",
        "        'decode_failures': np.nan,\n",
        "        'valid_rate': np.nan,\n",
        "        'srv_exact_match': np.nan,\n",
        "    }\n",
        "\n",
        "    m = re.search(r\"Samples requested:\\s*(\\d+)\", text)\n",
        "    if m:\n",
        "        out['samples_requested'] = int(m.group(1))\n",
        "\n",
        "    m = re.search(r\"Decoded circuits\\s*:\\s*(\\d+)\", text)\n",
        "    if m:\n",
        "        out['decoded_circuits'] = int(m.group(1))\n",
        "\n",
        "    m = re.search(r\"Decode failures\\s*:\\s*(\\d+)\", text)\n",
        "    if m:\n",
        "        out['decode_failures'] = int(m.group(1))\n",
        "\n",
        "    if not np.isnan(out['samples_requested']) and not np.isnan(out['decoded_circuits']):\n",
        "        out['valid_rate'] = out['decoded_circuits'] / out['samples_requested']\n",
        "\n",
        "    m = re.search(r\"SRV exact-match rate\\s*:\\s*([0-9.]+)\", text)\n",
        "    if m:\n",
        "        out['srv_exact_match'] = float(m.group(1))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def parse_training_losses(model_dir):\n",
        "    model_dir = Path(model_dir)\n",
        "    out = {\n",
        "        'train_loss_last': np.nan,\n",
        "        'train_loss_min': np.nan,\n",
        "        'valid_loss_last': np.nan,\n",
        "        'valid_loss_min': np.nan,\n",
        "    }\n",
        "\n",
        "    tr_path = model_dir / 'fit_losses.txt'\n",
        "    va_path = model_dir / 'fit_valid_losses.txt'\n",
        "\n",
        "    if tr_path.exists():\n",
        "        tr = np.loadtxt(tr_path)\n",
        "        if tr.size > 0:\n",
        "            out['train_loss_last'] = float(tr[-1])\n",
        "            out['train_loss_min'] = float(np.min(tr))\n",
        "\n",
        "    if va_path.exists():\n",
        "        va = np.loadtxt(va_path)\n",
        "        if va.ndim == 1 and va.size == 2:\n",
        "            va = va.reshape(1, 2)\n",
        "        if va.size > 0:\n",
        "            out['valid_loss_last'] = float(va[-1, 1])\n",
        "            out['valid_loss_min'] = float(np.min(va[:, 1]))\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rows = []\n",
        "\n",
        "for exp in EXPERIMENTS:\n",
        "    exp_name = exp['name']\n",
        "    model_name = f\"{exp_name}_{STAMP}\"\n",
        "    model_dir = REPO_ROOT / 'models' / 'trained' / model_name\n",
        "\n",
        "    # ----- Train -----\n",
        "    if RUN_TRAINING:\n",
        "        train_cmd = [\n",
        "            'python', str(TRAIN_SCRIPT),\n",
        "            f'training={TRAIN_BASE}',\n",
        "            f'general.model_name={model_name}',\n",
        "            f'training.model_name={model_name}',\n",
        "            f'general.experiment_name={model_name}',\n",
        "            f'training.experiment_name={model_name}',\n",
        "        ]\n",
        "        for k, v in exp['train_overrides'].items():\n",
        "            train_cmd.append(f'{k}={v}')\n",
        "\n",
        "        rc, out, err = run_cmd(train_cmd, dry_run=DRY_RUN)\n",
        "        (RESULTS_DIR / f'{model_name}_train_stdout.txt').write_text(out)\n",
        "        (RESULTS_DIR / f'{model_name}_train_stderr.txt').write_text(err)\n",
        "\n",
        "        if rc != 0:\n",
        "            rows.append({\n",
        "                'experiment': exp_name,\n",
        "                'model_name': model_name,\n",
        "                'guidance_scale': np.nan,\n",
        "                'status': 'train_failed',\n",
        "            })\n",
        "            continue\n",
        "\n",
        "    loss_stats = parse_training_losses(model_dir)\n",
        "\n",
        "    # ----- Eval sweep -----\n",
        "    if RUN_EVALUATION:\n",
        "        for g in GUIDANCE_SCALES:\n",
        "            eval_cmd = [\n",
        "                'python', str(EVAL_SCRIPT),\n",
        "                f'evaluation={EVAL_BASE}',\n",
        "                f'evaluation.model_dir={model_dir}',\n",
        "                f'evaluation.num_samples={NUM_EVAL_SAMPLES}',\n",
        "                f'evaluation.model_params.guidance_scale={g}',\n",
        "            ]\n",
        "\n",
        "            rc, out, err = run_cmd(eval_cmd, dry_run=DRY_RUN)\n",
        "            (RESULTS_DIR / f'{model_name}_eval_g{g}_stdout.txt').write_text(out)\n",
        "            (RESULTS_DIR / f'{model_name}_eval_g{g}_stderr.txt').write_text(err)\n",
        "\n",
        "            metrics = parse_eval_stdout(out)\n",
        "            rows.append({\n",
        "                'experiment': exp_name,\n",
        "                'model_name': model_name,\n",
        "                'guidance_scale': g,\n",
        "                'status': 'ok' if rc == 0 else 'eval_failed',\n",
        "                **loss_stats,\n",
        "                **metrics,\n",
        "            })\n",
        "\n",
        "results_df = pd.DataFrame(rows)\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not results_df.empty:\n",
        "    out_csv = RESULTS_DIR / f'results_{STAMP}.csv'\n",
        "    results_df.to_csv(out_csv, index=False)\n",
        "    print('Saved:', out_csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not results_df.empty:\n",
        "    display_cols = [\n",
        "        'experiment', 'model_name', 'guidance_scale', 'status',\n",
        "        'valid_rate', 'srv_exact_match',\n",
        "        'train_loss_last', 'train_loss_min', 'valid_loss_last', 'valid_loss_min'\n",
        "    ]\n",
        "    display(results_df[display_cols].sort_values(['experiment', 'guidance_scale']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not results_df.empty:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    for exp_name, grp in results_df.groupby('experiment'):\n",
        "        grp = grp.sort_values('guidance_scale')\n",
        "        axes[0].plot(grp['guidance_scale'], grp['valid_rate'], marker='o', label=exp_name)\n",
        "        axes[1].plot(grp['guidance_scale'], grp['srv_exact_match'], marker='o', label=exp_name)\n",
        "\n",
        "    axes[0].set_title('Decode Valid Rate vs Guidance')\n",
        "    axes[0].set_xlabel('guidance_scale')\n",
        "    axes[0].set_ylabel('valid_rate')\n",
        "    axes[0].set_ylim(0, 1)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[1].set_title('SRV Exact Match vs Guidance')\n",
        "    axes[1].set_xlabel('guidance_scale')\n",
        "    axes[1].set_ylabel('exact_match')\n",
        "    axes[1].set_ylim(0, 1)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[1].legend(loc='best', fontsize=8)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "- Keep `RUN_TRAINING=False` if you only want to re-run evaluation on existing `model_name`s.\n",
        "- If a run crashes, inspect `notebooks/results/overfit_training_comparison/*stderr.txt`.\n",
        "- Start with 2-3 experiments, then expand once validity moves above ~50% on 3q."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
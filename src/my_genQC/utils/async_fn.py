"""Basic functions for async executions."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../src/utils/async_fn.ipynb.

# %% auto 0
__all__ = ['run_parallel_jobs', 'MemoryMappedArray']

# %% ../../src/utils/async_fn.ipynb 2
from ..imports import *
from joblib import Parallel, delayed
from tqdm import tqdm
import math

from tensordict.tensordict import MemoryMappedTensor
import tempfile

# %% ../../src/utils/async_fn.ipynb 4
def _chunk_iterable(it, size: int):
    """Yield fixed-size chunks from any iterable without materializing the whole thing."""
    chunk = []
    for item in it:
        chunk.append(item)
        if len(chunk) == size:
            yield chunk
            chunk = []
    if chunk:
        yield chunk


def run_parallel_jobs(f: callable,
                      loop_set,
                      n_jobs: int = 1,
                      prefer: str = "processes",
                      batch_size: int | None = None,
                      progress: bool = False):
    """
    Run a function in parallel over an iterable.

    - prefer="processes" avoids the GIL for Python-heavy work (default).
    - batch_size groups items to amortize scheduling/pickling overhead.
    - progress adds a tqdm bar; if the iterable has no length, tqdm still shows a simple counter.
    """
    def _maybe_len(obj):
        try:
            return len(obj)
        except Exception:
            return None

    # fast path
    if n_jobs == 1:
        iterable = loop_set
        total = _maybe_len(loop_set)
        if progress:
            iterable = tqdm(iterable, total=total)
        return [f(x) for x in iterable]

    if batch_size:
        def _run_batch(batch):
            return [f(x) for x in batch]

        total_items = _maybe_len(loop_set)
        total_batches = None
        if total_items is not None:
            total_batches = math.ceil(total_items / batch_size)

        batches = _chunk_iterable(loop_set, batch_size)
        if progress:
            batches = tqdm(batches, total=total_batches)

        res_batches = Parallel(n_jobs=n_jobs,
                               prefer=prefer,
                               batch_size="auto")(delayed(_run_batch)(b) for b in batches)
        # flatten
        return [item for batch in res_batches for item in batch]

    iterable = loop_set
    total = _maybe_len(loop_set)
    if progress:
        iterable = tqdm(iterable, total=total)

    return Parallel(n_jobs=n_jobs,
                    prefer=prefer,
                    batch_size="auto")(delayed(f)(x) for x in iterable)

# %% ../../src/utils/async_fn.ipynb 6
class MemoryMappedArray():
    def __init__(self, obj, type="tensor"):
        self.obj  = obj
        self.type = type
        assert type in ["tensor", "numpy"]
        
        with tempfile.NamedTemporaryFile(delete=False) as file:   
            # Note can bes simplified with python 3.12 as we can set delete=true, and delete_on_close=True, so it will be kept and we dont need to delete
            # see https://docs.python.org/3.12/library/tempfile.html
            
            self.temporaryFileName = file.name
            file.close()

        if self.type == "numpy":
            self.obj_memmap    = np.memmap(filename=self.temporaryFileName, dtype=obj.dtype, mode='w+', shape=obj.shape)
            self.obj_memmap[:] = self.obj[:]
            self.obj_memmap.flush()
        
        elif self.type == "tensor":  
            self.obj_memmap = MemoryMappedTensor.from_tensor(self.obj.cpu(), filename=self.temporaryFileName, existsok=True) 
        
        else: 
            raise NotImplementedError()

    def get_obj(self):
        if self.type == "numpy":
            self.obj = self.obj_memmap.copy()
            
        elif self.type == "tensor":
            self.obj = self.obj_memmap.contiguous().clone().to(self.obj.device)

        del self.obj_memmap
        return self.obj, self.temporaryFileName

    @staticmethod
    def clean(temp_files):
        for temp_file in temp_files:
            try: os.remove(temp_file)
            except Exception as e: print(f"[ERROR]: {e}") 
